{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5458356d",
   "metadata": {},
   "source": [
    "# Preprocessing Pipeline\n",
    "\n",
    "Notebook version of `notebooks/preprocessing.py`.\n",
    "\n",
    "Run all cells to regenerate:\n",
    "- `data/processed/Training_Dataset_Preprocessed.csv`\n",
    "- `data/processed/Test_Dataset_Preprocessed.csv`\n",
    "- `data/processed/common_genes.txt`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6a41522e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "PREPROCESSING PIPELINE - ALL 7 DATASETS\n",
      "======================================================================\n",
      "\n",
      "Steps:\n",
      "1. Load GPL annotation files (probe -> gene mapping)\n",
      "2. Load GEO series matrix files (raw expression)\n",
      "3. Map probe IDs to gene symbols\n",
      "4. Log2 transformation\n",
      "5. Handle missing values\n",
      "6. Assign labels (0=Normal, 1=Tumor)\n",
      "7. Merge datasets by common genes\n",
      "======================================================================\n",
      "\n",
      "======================================================================\n",
      "PROCESSING TRAINING DATASETS (5 datasets)\n",
      "======================================================================\n",
      "\n",
      "[1/5] Processing training dataset: GSE24124\n",
      "\n",
      "Processing GSE24124...\n",
      "  Parsing GPL887-20438.txt...\n",
      "    Extracted 19168 probe-to-gene mappings\n",
      "  Loading GSE24124_series_matrix.txt...\n",
      "    Loaded 22153 probes x 119 samples\n",
      "    Mapped to 16546 unique genes\n",
      "    Final: 119 samples, 16546 genes\n",
      "    Labels: Tumor=99, Normal=20\n",
      "\n",
      "[2/5] Processing training dataset: GSE32641\n",
      "\n",
      "Processing GSE32641...\n",
      "  Parsing GPL887-20438.txt...\n",
      "    Extracted 19168 probe-to-gene mappings\n",
      "  Loading GSE32641_series_matrix.txt...\n",
      "    Loaded 22153 probes x 102 samples\n",
      "    Mapped to 16546 unique genes\n",
      "    Final: 102 samples, 16546 genes\n",
      "    Labels: Tumor=95, Normal=7\n",
      "\n",
      "[3/5] Processing training dataset: GSE36295\n",
      "\n",
      "Processing GSE36295...\n",
      "  Parsing GPL6244-17930.txt...\n",
      "    Extracted 25293 probe-to-gene mappings\n",
      "  Loading GSE36295_series_matrix.txt...\n",
      "    Loaded 33297 probes x 50 samples\n",
      "    Mapped to 23307 unique genes\n",
      "    Final: 50 samples, 23307 genes\n",
      "    Labels: Tumor=45, Normal=5\n",
      "\n",
      "[4/5] Processing training dataset: GSE42568\n",
      "\n",
      "Processing GSE42568...\n",
      "  Parsing GPL570-55999.txt...\n",
      "    Extracted 43509 probe-to-gene mappings\n",
      "  Loading GSE42568_series_matrix.txt...\n",
      "    Loaded 54675 probes x 121 samples\n",
      "    Mapped to 20024 unique genes\n",
      "    Final: 121 samples, 20024 genes\n",
      "    Labels: Tumor=104, Normal=17\n",
      "\n",
      "[5/5] Processing training dataset: GSE53752\n",
      "\n",
      "Processing GSE53752...\n",
      "  Parsing GPL7264-9589.txt...\n",
      "    Extracted 18703 probe-to-gene mappings\n",
      "  Loading GSE53752_series_matrix.txt...\n",
      "    Loaded 20130 probes x 76 samples\n",
      "    Mapped to 16717 unique genes\n",
      "    Final: 76 samples, 16717 genes\n",
      "    Labels: Tumor=51, Normal=25\n",
      "\n",
      "======================================================================\n",
      "PROCESSING TEST DATASETS (2 datasets)\n",
      "======================================================================\n",
      "\n",
      "[1/2] Processing test dataset: GSE70947\n",
      "\n",
      "Processing GSE70947...\n",
      "  Parsing GPL13607-20416.txt...\n",
      "    Extracted 62976 probe-to-gene mappings\n",
      "  Loading GSE70947_series_matrix.txt...\n",
      "    Loaded 62175 probes x 296 samples\n",
      "    Mapped to 34710 unique genes\n",
      "    Final: 296 samples, 34710 genes\n",
      "    Labels: Tumor=148, Normal=148\n",
      "\n",
      "[2/2] Processing test dataset: GSE109169\n",
      "\n",
      "Processing GSE109169...\n",
      "  Parsing GPL5175-3188.txt...\n",
      "    Extracted 33475 probe-to-gene mappings\n",
      "  Loading GSE109169_series_matrix.txt...\n",
      "    Loaded 19076 probes x 50 samples\n",
      "    Mapped to 17324 unique genes\n",
      "    Final: 50 samples, 17324 genes\n",
      "    Labels: Tumor=25, Normal=25\n",
      "\n",
      "======================================================================\n",
      "MERGING TRAINING DATASETS\n",
      "======================================================================\n",
      "\n",
      "======================================================================\n",
      "MERGING DATASETS\n",
      "======================================================================\n",
      "  GSE24124: 16546 genes\n",
      "  GSE32641: 16546 genes\n",
      "  GSE36295: 23307 genes\n",
      "  GSE42568: 20024 genes\n",
      "  GSE53752: 16717 genes\n",
      "\n",
      "  Common genes: 12505\n",
      "\n",
      "  Merged dataset: (468, 12509)\n",
      "  Samples: 468\n",
      "  Labels: {1: 394, 0: 74}\n",
      "  Datasets: {'GSE42568': 121, 'GSE24124': 119, 'GSE32641': 102, 'GSE53752': 76, 'GSE36295': 50}\n",
      "\n",
      "[SAVED] Training data: c:\\Users\\mmsid\\Downloads\\Breast-Cancer-Biomarkers-Discover-Using-ML-main\\data\\processed/Training_Dataset_Preprocessed.csv\n",
      "  Shape: (468, 12509)\n",
      "  Samples: 468\n",
      "\n",
      "======================================================================\n",
      "MERGING TEST DATASETS\n",
      "======================================================================\n",
      "\n",
      "======================================================================\n",
      "MERGING DATASETS\n",
      "======================================================================\n",
      "  GSE70947: 34710 genes\n",
      "  GSE109169: 17324 genes\n",
      "\n",
      "  Common genes: 16673\n",
      "\n",
      "  Merged dataset: (346, 16677)\n",
      "  Samples: 346\n",
      "  Labels: {0: 173, 1: 173}\n",
      "  Datasets: {'GSE70947': 296, 'GSE109169': 50}\n",
      "\n",
      "[SAVED] Test data: c:\\Users\\mmsid\\Downloads\\Breast-Cancer-Biomarkers-Discover-Using-ML-main\\data\\processed/Test_Dataset_Preprocessed.csv\n",
      "  Shape: (346, 11983)\n",
      "  Samples: 346\n",
      "  Common genes with training: 11979\n",
      "\n",
      "======================================================================\n",
      "PREPROCESSING COMPLETE!\n",
      "======================================================================\n",
      "\n",
      "All 7 datasets processed:\n",
      "  Training: 5 datasets, 468 samples\n",
      "  Test: 2 datasets, 346 samples\n",
      "  Common genes: 12505\n",
      "\n",
      "Files generated:\n",
      "  - Training_Dataset_Preprocessed.csv\n",
      "  - Test_Dataset_Preprocessed.csv\n",
      "  - common_genes.txt\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "PREPROCESSING PIPELINE DOCUMENTATION\n",
    "====================================\n",
    "\n",
    "This script documents the complete preprocessing steps:\n",
    "1. Load raw GEO series matrix files\n",
    "2. Load GPL annotation files  \n",
    "3. Map probe IDs to gene symbols\n",
    "4. Log2 transformation\n",
    "5. Handle missing values\n",
    "6. Merge datasets\n",
    "7. Assign labels\n",
    "\n",
    "Original preprocessing by project author - documented here for transparency\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from io import StringIO\n",
    "import re\n",
    "from pathlib import Path\n",
    "\n",
    "# Define file paths\n",
    "if \"__file__\" in globals():\n",
    "    PROJECT_ROOT = Path(__file__).resolve().parents[1]\n",
    "elif Path.cwd().name == \"notebooks\":\n",
    "    PROJECT_ROOT = Path.cwd().parent\n",
    "else:\n",
    "    PROJECT_ROOT = Path.cwd()\n",
    "\n",
    "RAW_DATA_PATH = str(PROJECT_ROOT / \"data\" / \"raw\") + \"/\"\n",
    "ANNOTATION_PATH = str(PROJECT_ROOT / \"data\" / \"annotation\") + \"/\"\n",
    "OUTPUT_PATH = str(PROJECT_ROOT / \"data\" / \"processed\") + \"/\"\n",
    "\n",
    "Path(OUTPUT_PATH).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Dataset to GPL mapping\n",
    "# Training datasets (5)\n",
    "TRAINING_DATASETS = {\n",
    "    'GSE24124': ('GPL887-20438.txt', 'GPL887'),\n",
    "    'GSE32641': ('GPL887-20438.txt', 'GPL887'),\n",
    "    'GSE36295': ('GPL6244-17930.txt', 'GPL6244'),\n",
    "    'GSE42568': ('GPL570-55999.txt', 'GPL570'),\n",
    "    'GSE53752': ('GPL7264-9589.txt', 'GPL7264')\n",
    "}\n",
    "\n",
    "# Test datasets (2)\n",
    "TEST_DATASETS = {\n",
    "    'GSE70947': ('GPL13607-20416.txt', 'GPL13607'),\n",
    "    'GSE109169': ('GPL5175-3188.txt', 'GPL5175')\n",
    "}\n",
    "\n",
    "# All datasets\n",
    "ALL_DATASETS = {**TRAINING_DATASETS, **TEST_DATASETS}\n",
    "\n",
    "def parse_gpl_file(gpl_file, platform_type):\n",
    "    \"\"\"\n",
    "    Parse GPL annotation file based on platform type\n",
    "    \"\"\"\n",
    "    print(f\"  Parsing {gpl_file}...\")\n",
    "    \n",
    "    if platform_type == 'GPL887':  # Agilent\n",
    "        gpl_df = pd.read_csv(ANNOTATION_PATH + gpl_file, sep=\"\\t\", comment=\"#\", low_memory=False)\n",
    "        probe_map = gpl_df[['ID', 'GENE_SYMBOL']].dropna()\n",
    "        \n",
    "    elif platform_type == 'GPL6244':  # Affy Gene 1.0 ST\n",
    "        gpl_df = pd.read_csv(ANNOTATION_PATH + gpl_file, sep=\"\\t\", comment=\"#\", low_memory=False)\n",
    "        def extract_gene(text):\n",
    "            if pd.isna(text): return None\n",
    "            parts = [p.strip() for p in str(text).split(\"//\")]\n",
    "            return parts[1] if len(parts) >= 2 and parts[1] != '---' else None\n",
    "        gpl_df['GENE_SYMBOL'] = gpl_df['gene_assignment'].apply(extract_gene)\n",
    "        probe_map = gpl_df[['ID', 'GENE_SYMBOL']].dropna()\n",
    "        \n",
    "    elif platform_type == 'GPL570':  # Affy U133 Plus 2.0\n",
    "        probe_ids, gene_symbols = [], []\n",
    "        with open(ANNOTATION_PATH + gpl_file, 'r', encoding='utf-8') as f:\n",
    "            for line in f:\n",
    "                if line.startswith((\"ID\", \"!\", \"#\")):\n",
    "                    continue\n",
    "                fields = line.strip().split('\\t')\n",
    "                if len(fields) < 2:\n",
    "                    continue\n",
    "                probe_id = fields[0]\n",
    "                line_text = ' '.join(fields)\n",
    "                match = re.search(r\"\\s([A-Z0-9\\-]{2,})\\s+\\d{1,6}\", line_text)\n",
    "                if match:\n",
    "                    gene = match.group(1)\n",
    "                    if gene.upper() not in [\"HOMO\", \"NA\"]:\n",
    "                        probe_ids.append(probe_id)\n",
    "                        gene_symbols.append(gene)\n",
    "        probe_map = pd.DataFrame({\"ID\": probe_ids, \"GENE_SYMBOL\": gene_symbols})\n",
    "        \n",
    "    elif platform_type == 'GPL7264':  # Affy Exon 1.0 ST\n",
    "        probe_ids, gene_symbols = [], []\n",
    "        with open(ANNOTATION_PATH + gpl_file, 'r', encoding='utf-8') as f:\n",
    "            for line in f:\n",
    "                if line.startswith((\"ID\", \"!\", \"#\")):\n",
    "                    continue\n",
    "                fields = line.strip().split('\\t')\n",
    "                if len(fields) < 7:\n",
    "                    continue\n",
    "                probe_id = fields[0]\n",
    "                gene = fields[6] if len(fields) > 6 else None\n",
    "                if gene and gene != '---':\n",
    "                    probe_ids.append(probe_id)\n",
    "                    gene_symbols.append(gene)\n",
    "        probe_map = pd.DataFrame({\"ID\": probe_ids, \"GENE_SYMBOL\": gene_symbols})\n",
    "        \n",
    "    elif platform_type == 'GPL13607':  # Affy Gene 2.0 ST\n",
    "        gpl_df = pd.read_csv(ANNOTATION_PATH + gpl_file, sep=\"\\t\", comment=\"#\", low_memory=False)\n",
    "        if 'GeneName' in gpl_df.columns:\n",
    "            probe_map = gpl_df[['ID', 'GeneName']].rename(columns={'GeneName': 'GENE_SYMBOL'}).dropna()\n",
    "        else:\n",
    "            probe_map = gpl_df[['ID', 'GENE_SYMBOL']].dropna()\n",
    "            \n",
    "    elif platform_type == 'GPL5175':  # Affy HuEx-1_0-st\n",
    "        gpl_df = pd.read_csv(ANNOTATION_PATH + gpl_file, sep=\"\\t\", comment=\"#\", low_memory=False)\n",
    "        def extract_first_gene(text):\n",
    "            if pd.isna(text):\n",
    "                return None\n",
    "            match = re.search(r'//\\s*([A-Za-z0-9\\-\\.]+)\\s*//', str(text))\n",
    "            return match.group(1) if match else None\n",
    "        gpl_df['GENE_SYMBOL'] = gpl_df['gene_assignment'].apply(extract_first_gene)\n",
    "        probe_map = gpl_df[['ID', 'GENE_SYMBOL']].dropna()\n",
    "    \n",
    "    else:\n",
    "        raise ValueError(f\"Unknown platform: {platform_type}\")\n",
    "    \n",
    "    probe_map = probe_map.copy()\n",
    "    probe_map[\"ID\"] = probe_map[\"ID\"].astype(str).str.strip()\n",
    "    probe_map[\"GENE_SYMBOL\"] = probe_map[\"GENE_SYMBOL\"].astype(str).str.strip()\n",
    "    probe_map = probe_map[(probe_map[\"GENE_SYMBOL\"] != \"\") & (probe_map[\"GENE_SYMBOL\"] != \"---\")]\n",
    "\n",
    "    print(f\"    Extracted {len(probe_map)} probe-to-gene mappings\")\n",
    "    return probe_map\n",
    "\n",
    "\n",
    "def load_series_matrix(series_file):\n",
    "    \"\"\"\n",
    "    Load GEO series matrix file\n",
    "    \"\"\"\n",
    "    print(f\"  Loading {series_file}...\")\n",
    "    \n",
    "    with open(RAW_DATA_PATH + series_file, \"r\", encoding=\"utf-8\") as f:\n",
    "        lines = f.readlines()\n",
    "    \n",
    "    # Extract sample titles for labels\n",
    "    sample_title_line = next((l for l in lines if l.startswith(\"!Sample_title\")), None)\n",
    "    if sample_title_line:\n",
    "        sample_titles = [title.lower().strip().replace('\"', '') \n",
    "                        for title in sample_title_line.strip().split(\"\\t\")[1:]]\n",
    "    else:\n",
    "        sample_titles = []\n",
    "    \n",
    "    # Extract GSM IDs\n",
    "    sample_id_line = next((l for l in lines if l.startswith(\"!Sample_geo_accession\")), None)\n",
    "    if sample_id_line:\n",
    "        gsm_ids = [s.strip().replace('\"', '') \n",
    "                  for s in sample_id_line.strip().split(\"\\t\")[1:]]\n",
    "    else:\n",
    "        gsm_ids = [f\"Sample_{i}\" for i in range(len(sample_titles))]\n",
    "    \n",
    "    # Read expression data\n",
    "    data_lines = [line for line in lines if not line.startswith(\"!\")]\n",
    "    expr_df = pd.read_csv(StringIO(\"\".join(data_lines)), sep=\"\\t\", low_memory=False)\n",
    "    expr_df = expr_df.rename(columns={expr_df.columns[0]: \"ProbeID\"})\n",
    "    expr_df['ProbeID'] = expr_df['ProbeID'].astype(str).str.strip()\n",
    "    expr_df = expr_df.set_index(\"ProbeID\")\n",
    "    expr_df = expr_df.apply(pd.to_numeric, errors='coerce')\n",
    "    \n",
    "    # Remove probes with >50% missing values\n",
    "    expr_df = expr_df.dropna(thresh=int(expr_df.shape[1] * 0.5))\n",
    "    \n",
    "    print(f\"    Loaded {expr_df.shape[0]} probes x {expr_df.shape[1]} samples\")\n",
    "    return expr_df, sample_titles, gsm_ids\n",
    "\n",
    "\n",
    "def assign_labels(sample_titles):\n",
    "    \"\"\"\n",
    "    Assign binary labels based on sample titles\n",
    "    0 = Normal, 1 = Tumor\n",
    "    \"\"\"\n",
    "    labels = []\n",
    "    disease_types = []\n",
    "    valid_indices = []\n",
    "    \n",
    "    for i, title in enumerate(sample_titles):\n",
    "        title_lower = title.lower()\n",
    "        \n",
    "        # Check for normal/control\n",
    "        if any(term in title_lower for term in ['normal', 'control', 'nontumor', 'healthy', 'adjacent']):\n",
    "            labels.append(0)\n",
    "            disease_types.append('normal')\n",
    "            valid_indices.append(i)\n",
    "        # Check for tumor/cancer\n",
    "        elif any(term in title_lower for term in ['tumor', 'cancer', 'carcinoma', 'idc', 'malignant']):\n",
    "            labels.append(1)\n",
    "            disease_types.append('tumor')\n",
    "            valid_indices.append(i)\n",
    "    \n",
    "    return labels, disease_types, valid_indices\n",
    "\n",
    "\n",
    "def preprocess_dataset(dataset_name, gpl_file, platform_type):\n",
    "    \"\"\"\n",
    "    Preprocess a single dataset\n",
    "    \"\"\"\n",
    "    print(f\"\\nProcessing {dataset_name}...\")\n",
    "    \n",
    "    # 1. Load GPL annotation\n",
    "    probe_map = parse_gpl_file(gpl_file, platform_type)\n",
    "    \n",
    "    # 2. Load series matrix\n",
    "    series_file = f\"{dataset_name}_series_matrix.txt\"\n",
    "    expr_df, sample_titles, gsm_ids = load_series_matrix(series_file)\n",
    "    \n",
    "    # 3. Map probes to genes\n",
    "    common_probes = expr_df.index.intersection(probe_map['ID'].astype(str).values)\n",
    "    expr_subset = expr_df.loc[common_probes]\n",
    "    probe_to_gene = dict(zip(probe_map['ID'], probe_map['GENE_SYMBOL']))\n",
    "    expr_subset['GeneSymbol'] = expr_subset.index.map(probe_to_gene)\n",
    "    expr_subset = expr_subset.dropna(subset=['GeneSymbol'])\n",
    "    gene_expr = expr_subset.groupby('GeneSymbol').mean()\n",
    "    \n",
    "    print(f\"    Mapped to {len(gene_expr)} unique genes\")\n",
    "    \n",
    "    # 4. Transpose so samples are rows\n",
    "    gene_expr = gene_expr.T\n",
    "    \n",
    "    # 5. Assign labels\n",
    "    labels, disease_types, valid_indices = assign_labels(sample_titles)\n",
    "    gene_expr = gene_expr.iloc[valid_indices]\n",
    "    \n",
    "    # 6. Add metadata\n",
    "    gene_expr['GSM_ID'] = [gsm_ids[i] for i in valid_indices]\n",
    "    gene_expr['DiseaseType'] = disease_types\n",
    "    gene_expr['label'] = labels\n",
    "    gene_expr['Dataset'] = dataset_name\n",
    "    \n",
    "    # 7. Log2 transform if needed\n",
    "    gene_cols = [c for c in gene_expr.columns if c not in ['GSM_ID', 'DiseaseType', 'label', 'Dataset']]\n",
    "    if gene_expr[gene_cols].max().max() > 100:\n",
    "        print(f\"    Applying log2 transformation...\")\n",
    "        gene_expr[gene_cols] = np.log2(gene_expr[gene_cols] + 1)\n",
    "    \n",
    "    print(f\"    Final: {gene_expr.shape[0]} samples, {len(gene_cols)} genes\")\n",
    "    print(f\"    Labels: Tumor={sum(labels)}, Normal={len(labels)-sum(labels)}\")\n",
    "    \n",
    "    return gene_expr\n",
    "\n",
    "\n",
    "def merge_datasets(datasets):\n",
    "    \"\"\"\n",
    "    Merge all datasets by common genes\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"MERGING DATASETS\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # Find common genes\n",
    "    meta_cols = ['GSM_ID', 'DiseaseType', 'label', 'Dataset']\n",
    "    gene_sets = []\n",
    "    \n",
    "    for name, df in datasets.items():\n",
    "        genes = [c for c in df.columns if c not in meta_cols]\n",
    "        gene_sets.append(set(genes))\n",
    "        print(f\"  {name}: {len(genes)} genes\")\n",
    "    \n",
    "    # Intersection\n",
    "    common_genes = gene_sets[0]\n",
    "    for gene_set in gene_sets[1:]:\n",
    "        common_genes = common_genes.intersection(gene_set)\n",
    "    \n",
    "    common_genes = sorted(list(common_genes))\n",
    "    print(f\"\\n  Common genes: {len(common_genes)}\")\n",
    "    \n",
    "    # Merge\n",
    "    merged = []\n",
    "    for name, df in datasets.items():\n",
    "        cols = meta_cols + common_genes\n",
    "        merged.append(df[cols])\n",
    "    \n",
    "    merged_df = pd.concat(merged, ignore_index=True)\n",
    "    \n",
    "    print(f\"\\n  Merged dataset: {merged_df.shape}\")\n",
    "    print(f\"  Samples: {len(merged_df)}\")\n",
    "    print(f\"  Labels: {merged_df['label'].value_counts().to_dict()}\")\n",
    "    print(f\"  Datasets: {merged_df['Dataset'].value_counts().to_dict()}\")\n",
    "    \n",
    "    return merged_df, common_genes\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Main preprocessing pipeline for all 7 datasets\n",
    "    \"\"\"\n",
    "    print(\"=\"*70)\n",
    "    print(\"PREPROCESSING PIPELINE - ALL 7 DATASETS\")\n",
    "    print(\"=\"*70)\n",
    "    print(\"\\nSteps:\")\n",
    "    print(\"1. Load GPL annotation files (probe -> gene mapping)\")\n",
    "    print(\"2. Load GEO series matrix files (raw expression)\")\n",
    "    print(\"3. Map probe IDs to gene symbols\")\n",
    "    print(\"4. Log2 transformation\")\n",
    "    print(\"5. Handle missing values\")\n",
    "    print(\"6. Assign labels (0=Normal, 1=Tumor)\")\n",
    "    print(\"7. Merge datasets by common genes\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"PROCESSING TRAINING DATASETS (5 datasets)\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # Process training datasets\n",
    "    train_datasets = {}\n",
    "    for dataset, (gpl_file, platform) in TRAINING_DATASETS.items():\n",
    "        try:\n",
    "            print(f\"\\n[{len(train_datasets)+1}/5] Processing training dataset: {dataset}\")\n",
    "            df = preprocess_dataset(dataset, gpl_file, platform)\n",
    "            train_datasets[dataset] = df\n",
    "        except Exception as e:\n",
    "            print(f\"  ERROR: {e}\")\n",
    "            continue\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"PROCESSING TEST DATASETS (2 datasets)\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # Process test datasets\n",
    "    test_datasets = {}\n",
    "    for dataset, (gpl_file, platform) in TEST_DATASETS.items():\n",
    "        try:\n",
    "            print(f\"\\n[{len(test_datasets)+1}/2] Processing test dataset: {dataset}\")\n",
    "            df = preprocess_dataset(dataset, gpl_file, platform)\n",
    "            test_datasets[dataset] = df\n",
    "        except Exception as e:\n",
    "            print(f\"  ERROR: {e}\")\n",
    "            continue\n",
    "    \n",
    "    # Merge training datasets\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"MERGING TRAINING DATASETS\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    if train_datasets:\n",
    "        train_merged, common_genes = merge_datasets(train_datasets)\n",
    "        \n",
    "        # Save training data\n",
    "        train_output = OUTPUT_PATH + \"Training_Dataset_Preprocessed.csv\"\n",
    "        train_merged.set_index('GSM_ID').to_csv(train_output)\n",
    "        \n",
    "        print(f\"\\n[SAVED] Training data: {train_output}\")\n",
    "        print(f\"  Shape: {train_merged.shape}\")\n",
    "        print(f\"  Samples: {len(train_merged)}\")\n",
    "    \n",
    "    # Merge test datasets\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"MERGING TEST DATASETS\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    if test_datasets:\n",
    "        # Find common genes with training data\n",
    "        test_merged, _ = merge_datasets(test_datasets)\n",
    "        \n",
    "        # Keep only genes present in training data\n",
    "        meta_cols = ['GSM_ID', 'DiseaseType', 'label', 'Dataset']\n",
    "        train_genes = [c for c in train_merged.columns if c not in meta_cols]\n",
    "        available_genes = [g for g in train_genes if g in test_merged.columns]\n",
    "        \n",
    "        test_merged_filtered = test_merged[meta_cols + available_genes]\n",
    "        \n",
    "        # Save test data\n",
    "        test_output = OUTPUT_PATH + \"Test_Dataset_Preprocessed.csv\"\n",
    "        test_merged_filtered.set_index('GSM_ID').to_csv(test_output)\n",
    "        \n",
    "        print(f\"\\n[SAVED] Test data: {test_output}\")\n",
    "        print(f\"  Shape: {test_merged_filtered.shape}\")\n",
    "        print(f\"  Samples: {len(test_merged_filtered)}\")\n",
    "        print(f\"  Common genes with training: {len(available_genes)}\")\n",
    "    \n",
    "    # Save gene list\n",
    "    with open(OUTPUT_PATH + \"common_genes.txt\", \"w\") as f:\n",
    "        for gene in common_genes:\n",
    "            f.write(gene + \"\\n\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"PREPROCESSING COMPLETE!\")\n",
    "    print(\"=\"*70)\n",
    "    print(\"\\nAll 7 datasets processed:\")\n",
    "    print(f\"  Training: {len(train_datasets)} datasets, {len(train_merged) if train_datasets else 0} samples\")\n",
    "    print(f\"  Test: {len(test_datasets)} datasets, {len(test_merged_filtered) if test_datasets else 0} samples\")\n",
    "    print(f\"  Common genes: {len(common_genes)}\")\n",
    "    print(\"\\nFiles generated:\")\n",
    "    print(\"  - Training_Dataset_Preprocessed.csv\")\n",
    "    print(\"  - Test_Dataset_Preprocessed.csv\")\n",
    "    print(\"  - common_genes.txt\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    return train_datasets, test_datasets\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    train_datasets, test_datasets = main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
